{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text and lowercase it\n",
    "filename = \"jane_virgin.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  22032\n",
      "Total Vocab:  47\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  21932\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21932/21932 [==============================] - 45s 2ms/step - loss: 3.1007\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.10067, saving model to weights-improvement-01-3.1007.hdf5\n",
      "Epoch 2/20\n",
      "21932/21932 [==============================] - 47s 2ms/step - loss: 3.0447\n",
      "\n",
      "Epoch 00002: loss improved from 3.10067 to 3.04466, saving model to weights-improvement-02-3.0447.hdf5\n",
      "Epoch 3/20\n",
      "21932/21932 [==============================] - 58s 3ms/step - loss: 3.0168\n",
      "\n",
      "Epoch 00003: loss improved from 3.04466 to 3.01684, saving model to weights-improvement-03-3.0168.hdf5\n",
      "Epoch 4/20\n",
      "21932/21932 [==============================] - 51s 2ms/step - loss: 2.9290\n",
      "\n",
      "Epoch 00004: loss improved from 3.01684 to 2.92905, saving model to weights-improvement-04-2.9290.hdf5\n",
      "Epoch 5/20\n",
      "21932/21932 [==============================] - 52s 2ms/step - loss: 2.8838\n",
      "\n",
      "Epoch 00005: loss improved from 2.92905 to 2.88384, saving model to weights-improvement-05-2.8838.hdf5\n",
      "Epoch 6/20\n",
      "21932/21932 [==============================] - 53s 2ms/step - loss: 2.8483\n",
      "\n",
      "Epoch 00006: loss improved from 2.88384 to 2.84827, saving model to weights-improvement-06-2.8483.hdf5\n",
      "Epoch 7/20\n",
      "21932/21932 [==============================] - 51s 2ms/step - loss: 2.8200\n",
      "\n",
      "Epoch 00007: loss improved from 2.84827 to 2.82004, saving model to weights-improvement-07-2.8200.hdf5\n",
      "Epoch 8/20\n",
      "21932/21932 [==============================] - 51s 2ms/step - loss: 2.8001\n",
      "\n",
      "Epoch 00008: loss improved from 2.82004 to 2.80006, saving model to weights-improvement-08-2.8001.hdf5\n",
      "Epoch 9/20\n",
      "21932/21932 [==============================] - 51s 2ms/step - loss: 2.7839\n",
      "\n",
      "Epoch 00009: loss improved from 2.80006 to 2.78391, saving model to weights-improvement-09-2.7839.hdf5\n",
      "Epoch 10/20\n",
      "21932/21932 [==============================] - 49s 2ms/step - loss: 2.7647\n",
      "\n",
      "Epoch 00010: loss improved from 2.78391 to 2.76470, saving model to weights-improvement-10-2.7647.hdf5\n",
      "Epoch 11/20\n",
      "21932/21932 [==============================] - 54s 2ms/step - loss: 2.7474\n",
      "\n",
      "Epoch 00011: loss improved from 2.76470 to 2.74736, saving model to weights-improvement-11-2.7474.hdf5\n",
      "Epoch 12/20\n",
      "21932/21932 [==============================] - 50s 2ms/step - loss: 2.7271\n",
      "\n",
      "Epoch 00012: loss improved from 2.74736 to 2.72713, saving model to weights-improvement-12-2.7271.hdf5\n",
      "Epoch 13/20\n",
      "21932/21932 [==============================] - 50s 2ms/step - loss: 2.7054\n",
      "\n",
      "Epoch 00013: loss improved from 2.72713 to 2.70536, saving model to weights-improvement-13-2.7054.hdf5\n",
      "Epoch 14/20\n",
      "21932/21932 [==============================] - 51s 2ms/step - loss: 2.6848\n",
      "\n",
      "Epoch 00014: loss improved from 2.70536 to 2.68478, saving model to weights-improvement-14-2.6848.hdf5\n",
      "Epoch 15/20\n",
      "21932/21932 [==============================] - 53s 2ms/step - loss: 2.6623\n",
      "\n",
      "Epoch 00015: loss improved from 2.68478 to 2.66230, saving model to weights-improvement-15-2.6623.hdf5\n",
      "Epoch 16/20\n",
      "21932/21932 [==============================] - 52s 2ms/step - loss: 2.6375\n",
      "\n",
      "Epoch 00016: loss improved from 2.66230 to 2.63752, saving model to weights-improvement-16-2.6375.hdf5\n",
      "Epoch 17/20\n",
      "21932/21932 [==============================] - 57s 3ms/step - loss: 2.6176\n",
      "\n",
      "Epoch 00017: loss improved from 2.63752 to 2.61758, saving model to weights-improvement-17-2.6176.hdf5\n",
      "Epoch 18/20\n",
      "21932/21932 [==============================] - 59s 3ms/step - loss: 2.5871\n",
      "\n",
      "Epoch 00018: loss improved from 2.61758 to 2.58711, saving model to weights-improvement-18-2.5871.hdf5\n",
      "Epoch 19/20\n",
      "21932/21932 [==============================] - 56s 3ms/step - loss: 2.5617\n",
      "\n",
      "Epoch 00019: loss improved from 2.58711 to 2.56173, saving model to weights-improvement-19-2.5617.hdf5\n",
      "Epoch 20/20\n",
      "21932/21932 [==============================] - 57s 3ms/step - loss: 2.5336\n",
      "\n",
      "Epoch 00020: loss improved from 2.56173 to 2.53357, saving model to weights-improvement-20-2.5336.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12d963b70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small LTSM Network to Generate Text for Jane the Virgin\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"lorena_telenovela.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-20-2.5336.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  for me wanting to end the pregnancy are so selfish, that i'm not ready, that this wasn't the plan,  \"\n",
      "i monn  ant thu re tor rore you reae  aat ao wou rene to toe wore  aat toe woar  i monn  ant thu re tor rore you reae  aat ao wou rene to toe wore  aat toe woar  i monn  ant thu re tor rore you reae  aat ao wou rene to toe wore  aat toe woar  i monn  ant thu re tor rore you reae  aat ao wou rene to toe wore  aat toe woar  i monn  ant thu re tor rore you reae  aat ao wou rene to toe wore  aat toe woar  i monn  ant thu re tor rore you reae  aat ao wou rene to toe wore  aat toe woar  i monn  ant thu re tor rore you reae  aat ao wou rene to toe wore  aat toe woar  i monn  ant thu re tor rore you reae  aat ao wou rene to toe wore  aat toe woar  i monn  ant thu re tor rore you reae  aat ao wou rene to toe wore  aat toe woar  i monn  ant thu re tor rore you reae  aat ao wou rene to toe wore  aat toe woar  i monn  ant thu re tor rore you reae  aat ao wou rene to toe wore  aat toe woar  i monn  ant thu re tor rore you reae  aat ao wou rene to toe wore  aat toe woar  i monn  ant thu re tor rore \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
